{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5ecc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bfd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_names_path = os.path.join(\"yeast_names.txt\")\n",
    "yeast_data_path = os.path.join(\"yeast_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f614b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RandomForest ===\n",
      "Accuracy: 0.6364, F1: 0.6291\n",
      "\n",
      "=== GradientBoosting ===\n",
      "Accuracy: 0.5926, F1: 0.5879\n",
      "\n",
      "=== XGBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya-ladawa/Desktop/Msc. Curriculum and stuff/PythonLab/psp/myenv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:34:02] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5791, F1: 0.5724\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy: 0.4209, F1: 0.3987\n",
      "\n",
      "=== SVM ===\n",
      "Accuracy: 0.5892, F1: 0.5939\n",
      "\n",
      "=== LogisticRegression ===\n",
      "Accuracy: 0.5253, F1: 0.5412\n",
      "\n",
      "=== KNN ===\n",
      "Accuracy: 0.5589, F1: 0.5510\n",
      "\n",
      "=== NaiveBayes ===\n",
      "Accuracy: 0.1279, F1: 0.1405\n",
      "\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy        F1\n",
      "0        RandomForest  0.636364  0.629075\n",
      "4                 SVM  0.589226  0.593859\n",
      "1    GradientBoosting  0.592593  0.587898\n",
      "2             XGBoost  0.579125  0.572432\n",
      "6                 KNN  0.558923  0.550994\n",
      "5  LogisticRegression  0.525253  0.541239\n",
      "3            AdaBoost  0.420875  0.398721\n",
      "7          NaiveBayes  0.127946  0.140455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84939/3229093169.py:121: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"F1\", y=\"Model\", data=res_df, palette=\"vlag\")\n",
      "/tmp/ipykernel_84939/3229093169.py:128: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Accuracy\", y=\"Model\", data=res_df, palette=\"rocket\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model (RandomForest) saved to best_model.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, f1_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Ensure output directory\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "YEAST_PATH = yeast_data_path  # adjust as needed\n",
    "columns = [\n",
    "    \"Sequence_Name\", \"mcg\", \"gvh\", \"alm\",\n",
    "    \"mit\", \"erl\", \"pox\", \"vac\", \"nuc\",\n",
    "    \"Localization_Site\"\n",
    "]\n",
    "df = pd.read_csv(YEAST_PATH, sep=r\"\\s+\", names=columns)\n",
    "\n",
    "# --- 2. ENCODE TARGET ---\n",
    "X = df.drop(columns=[\"Sequence_Name\", \"Localization_Site\"] )\n",
    "y = df[\"Localization_Site\"]\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# --- 3. SPLIT DATA ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# --- 4. BASIC EDA (save plots) ---\n",
    "# Class distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x=y, order=y.value_counts().index)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/class_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# Feature distributions\n",
    "plt.figure(figsize=(10,6))\n",
    "X.hist(bins=15, figsize=(10,6))\n",
    "plt.suptitle(\"Feature Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/feature_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(X.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/feature_correlation.png\")\n",
    "plt.close()\n",
    "\n",
    "# PCA scatter\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "comps = pca.fit_transform(X_scaled)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=comps[:,0], y=comps[:,1], hue=y, palette='tab10', legend=False)\n",
    "plt.title(\"PCA (2 Components)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/pca_scatter.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 5. DEFINE MODELS ---\n",
    "seed = 42\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=1500, random_state=seed, class_weight=\"balanced\"),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=seed),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=seed, tree_method='hist', n_jobs=-1,         predictor=\"cpu_predictor\",),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=seed),\n",
    "    \"SVM\": SVC(probability=True, random_state=seed, class_weight=\"balanced\"),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=seed, class_weight=\"balanced\"),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"NaiveBayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# --- 6. TRAIN & PREDICT ---\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"=== {name} ===\")\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}\\n\")\n",
    "    results.append({\"Model\": name, \"Accuracy\": acc, \"F1\": f1, \"Pipeline\": pipe})\n",
    "\n",
    "# --- 7. METRICS & COMPARISON ---\n",
    "res_df = pd.DataFrame(results).sort_values(\"F1\", ascending=False)\n",
    "print(\"\\nModel Comparison:\\n\", res_df[[\"Model\",\"Accuracy\",\"F1\"]])\n",
    "\n",
    "# Save comparison plots\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=\"F1\", y=\"Model\", data=res_df, palette=\"vlag\")\n",
    "plt.title(\"Model F1 Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/model_f1_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=\"Accuracy\", y=\"Model\", data=res_df, palette=\"rocket\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/model_accuracy_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 8. CONFUSION & ROC for BEST MODEL ---\n",
    "best = res_df.iloc[0]\n",
    "best_name, best_pipe = best[\"Model\"], best[\"Pipeline\"]\n",
    "y_best = best_pipe.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_best)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f\"Confusion Matrix: {best_name}\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/confusion_matrix_best.png\")\n",
    "plt.close()\n",
    "\n",
    "# ROC curves (multiclass)\n",
    "y_test_bin = label_binarize(y_test, classes=range(len(le.classes_)))\n",
    "y_score = best_pipe.predict_proba(X_test)\n",
    "\n",
    "fpr = dict(); tpr = dict(); roc_auc = dict()\n",
    "for i in range(len(le.classes_)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Macro-average\n",
    "all_fpr = sorted({x for vals in fpr.values() for x in vals})\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in fpr:\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= len(le.classes_)\n",
    "fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    plt.plot(fpr[i], tpr[i], lw=1,\n",
    "             label=f\"{cls} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', lw=2,\n",
    "         label=f\"macro-average (AUC = {roc_auc['macro']:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', lw=1, color='gray')\n",
    "plt.xlim([0.0,1.0]); plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title(f\"ROC Curves: {best_name}\")\n",
    "plt.legend(loc='lower right', fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/roc_curves_best.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 9. SAVE BEST MODEL ---\n",
    "with open(\"best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_pipe, f)\n",
    "print(f\"Best model ({best_name}) saved to best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5726f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. INFERENCE FUNCTIONS ---\n",
    "from typing import List\n",
    "\n",
    "def predict_from_input(feature_list: List[List[float]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict localization sites for provided feature vectors.\n",
    "\n",
    "    Args:\n",
    "        feature_list: List of feature lists in the order [mcg, gvh, alm, mit, erl, pox, vac, nuc].\n",
    "    Returns:\n",
    "        List of predicted localization site labels.\n",
    "    \"\"\"\n",
    "    # Load model and label encoder\n",
    "    with open(\"best_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    preds_enc = model.predict(feature_list)\n",
    "    return le.inverse_transform(preds_enc)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def predict_random(n: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sample n random rows from the original dataset and predict their localization.\n",
    "\n",
    "    Args:\n",
    "        n: Number of random samples to predict.\n",
    "    Returns:\n",
    "        DataFrame containing original features, true labels, and predicted labels.\n",
    "    \"\"\"\n",
    "    # draw n different random samples each call\n",
    "    samples = df.sample(n=n).reset_index(drop=True)\n",
    "    X_rand = samples.drop(columns=[\"Sequence_Name\", \"Localization_Site\"])\n",
    "    preds_enc = best_pipe.predict(X_rand)\n",
    "    samples[\"Predicted_Site\"] = le.inverse_transform(preds_enc)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c05a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence_Name   mcg  gvh   alm   mit  erl  pox   vac   nuc  \\\n",
      "0    NAB1_YEAST  0.49  0.6  0.55  0.17  0.5  0.0  0.44  0.22   \n",
      "1    DBF4_YEAST  0.35  0.4  0.59  0.22  0.5  0.0  0.47  0.37   \n",
      "\n",
      "  Localization_Site Predicted_Site  \n",
      "0               CYT            CYT  \n",
      "1               NUC            NUC  \n"
     ]
    }
   ],
   "source": [
    "# # Example usages:\n",
    "# # 1) Custom input prediction:\n",
    "# custom = [[0.58, 0.61, 0.47, 0.13, 0.50, 0.00, 0.48, 0.22]]\n",
    "# print(predict_from_input(custom))\n",
    "\n",
    "# 2) Random sampling prediction:\n",
    "print(predict_random(n=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75331f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f07ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
